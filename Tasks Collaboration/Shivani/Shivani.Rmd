---
title: "RQ3"
output: html_document
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#Final Analysis

__3. What is the maximum likelihood of a customer to select a particular shop?__ <br />

__Algorithms selected:__ Naive Bayes, decision trees

__Reason for Algorithm Selections:__ Naive Bayes is a simple Bayesian supervised classiÔ¨Åer that assumes that all attributes are independent of each other.Because of this all attributes can be learned separately which results in faster performance.But its accuracy rate is lesser than that of Decision trees.Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems.A small change in the data can cause a large change in the final estimated tree.However,they are intuitively very easy to explain. They closely mirror human decision-making compared to other regression and classification approaches.

__Features Selected:__ distance_avg,products_purchased_avg,unique_products_purchased_avg,product_price_Avg, amount_purchased_Avg.

__Analysis__
__1) Data preparation for classification__
```{r echo=TRUE, message=FALSE}
library(caret)
library(dplyr)         # Used by caret
library(kernlab)       # support vector machine 
library(pROC)	
library(e1071)
library(rpart)
library(readr)
library(tidyverse)
library(rpart.plot)

file_path<- "Supermarket_Data_Prediction.csv"
supermarket_data_predict <- read_csv(file_path)
View(supermarket_data_predict)
supermarket_data_predict$most_pref_shop=factor(supermarket_data_predict$most_pref_shop)
str(supermarket_data_predict)
```

__2) Stratified k-fold cross validation__

```{r echo=TRUE, message=FALSE}
#stratified k-fold(5)
set.seed(123)
folds <- cut(seq(1,nrow(supermarket_data_predict)),breaks=5,labels=FALSE)
```
__3)  Accuracy calculation__
```{r echo=TRUE, message=FALSE}
#to store accuracy,sensitivity ,specificity for naive bayes and decision tree
resnb<-matrix(ncol=3, nrow=5)
resdec<-matrix(ncol=3, nrow=5)
#5 folds 5 different train-test dataset combinations
for(i in 1:5){
  #Segement your data by fold using the which() function 
  
  testIndexes <- which(folds==i,arr.ind=TRUE)
  test <- supermarket_data_predict[testIndexes, ]
  train <- supermarket_data_predict[-testIndexes, ]
  ytrain<-train%>%pull(most_pref_shop)
  ytest<-test%>%pull(most_pref_shop)
  #function to calculate accuracy of different models
  printALL=function(model,name,result){
    print(name)
    testPred=predict(model, newdata=test, type="class")
    conftest<-confusionMatrix(ytest,testPred,"YES")
    print(conftest$overall["Accuracy"])
    #print(conftest)
    result[i,1]= conftest$overall["Accuracy"]
    print("Sensitivity")
    print(max(conftest$byClass[,"Sensitivity"]))
    result[i,2]=max(conftest$byClass[,"Sensitivity"])
    print("Specificity")
    print(max(conftest$byClass[,"Specificity"]))
    result[i,3]=max(conftest$byClass[,"Specificity"])
    return(result)
  }
  
  NBclassfier=naiveBayes(most_pref_shop ~., data=train,laplace=3)
  modelr<-rpart(most_pref_shop ~., data=train, method="class",control=rpart.control(cp=0.0001))
  resnb=printALL(NBclassfier,"naive bayes",resnb)
  resdec=printALL(modelr,"decision trees",resdec)
  
  
}
```
__4) Plotting model's performance__
```{r echo=TRUE, message=FALSE}
#cross-validation accuracy plot
y<-list(resnb[1,1],resnb[2,1],resnb[3,1],resnb[4,1],resnb[5,1])
ynew<-list(resdec[1,1],resdec[2,1],resdec[3,1],resdec[4,1],resdec[5,1])
x<-list(1,2,3,4,5)
xaxis<-unlist(x)
plot(xaxis,unlist(y),type="l" ,lwd=2,col="red",xlab="Folds",ylab="Accuracy",ylim=range( c(y, ynew) ),main="Cross Validation - Accuracy")
lines(xaxis,unlist(ynew),type="l",lwd=2,col="green")
legend("topleft", 
       legend = c("Naive Bayes", "Decision Tree"),lwd=2,col=c("red","green"))

#cross-validation sensitivity plot
y<-list(resnb[1,2],resnb[2,2],resnb[3,2],resnb[4,2],resnb[5,2])
ynew<-list(resdec[1,2],resdec[2,2],resdec[3,2],resdec[4,2],resdec[5,2])
x<-list(1,2,3,4,5)
xaxis<-unlist(x)
plot(xaxis,unlist(y),type="l" ,lwd=2,col="red",xlab="Folds",ylab="Sensitivity",ylim=range( c(y, ynew) ),main="Cross Validation - Sensitivity")
lines(xaxis,unlist(ynew),type="l",lwd=2,col="green")
legend("topleft", 
       legend = c("Naive Bayes", "Decision Tree"),lwd=2,col=c("red","green"))

#cross-validation specificity plot
y<-list(resnb[1,3],resnb[2,3],resnb[3,3],resnb[4,3],resnb[5,3])
ynew<-list(resdec[1,3],resdec[2,3],resdec[3,3],resdec[4,3],resdec[5,3])
x<-list(1,2,3,4,5)
xaxis<-unlist(x)
plot(xaxis,unlist(y),type="l" ,lwd=2,col="red",xlab="Folds",ylab="Specificity",ylim=range( c(y, ynew) ),main="Cross Validation - Specificity")
lines(xaxis,unlist(ynew),type="l",lwd=2,col="green")
legend("topleft", 
       legend = c("Naive Bayes", "Decision Tree"),lwd=2,col=c("red","green"))



```
## Classification
__5) Naive Bayes __
```{r echo=TRUE, message=FALSE}
#hypertuning isn't recommended for naive bayes because the overall performance is affected by just one parameter i.e laplace
NBclassfier=naiveBayes(most_pref_shop ~., data=train,laplace=3)
print(NBclassfier)


```

__6) Decision tree (rpart) __
```{r echo=TRUE, message=FALSE}
#hypertuning rpart
obj3 <- tune.rpart(most_pref_shop~., data =train, minsplit = c(5,10,15))
summary(obj3)
#decision tree classifier
modelr<-rpart(most_pref_shop ~., data=train, method="class",control=rpart.control(cp=0.001))
print(modelr$cptable)
plotcp(modelr)
prp(modelr)

#the relative error is reduced but the graph is overplotted
modelr<-rpart(most_pref_shop ~., data=train, method="class",control=rpart.control(cp=0.0001))
print(modelr$cptable)
```
__Conclusion:__
By considering the features distance_avg,products_purchased_avg,unique_products_purchased_avg,product_price_Avg, amount_purchased_Avg, most preferred shop is calculated and the above mentioned classification algorithms are applied.This will further help in predicting which shop the customer would choose