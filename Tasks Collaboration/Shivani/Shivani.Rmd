---
title: "RQ3"
author:"Shivani Jadhav"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

#Final Analysis

__3. What is the maximum likelihood of a customer to select a particular shop?__ <br />

__Algorithms selected:__ Naive Bayes, decision trees

__Reason for Algorithm Selections:__ Naive Bayes is a simple Bayesian supervised classiÔ¨Åer that assumes that all attributes are independent of each other.Because of this all attributes can be learned separately which results in faster performance.But its accuracy rate is lesser than that of Decision trees.Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems.A small change in the data can cause a large change in the final estimated tree.However,they are intuitively very easy to explain. They closely mirror human decision-making compared to other regression and classification approaches.

__Features Selected:__ distance_avg,products_purchased_avg,unique_products_purchased_avg,product_price_Avg, amount_purchased_Avg.

__Analysis__
__1) Data preparation for classification__
```{r echo=TRUE, message=FALSE}
library(caret)
library(dplyr)         # Used by caret
library(kernlab)       # support vector machine 
library(pROC)	
library(e1071)
library(rpart)
library(readr)
library(tidyverse)

file_path<- "Input Dataset/Cleaned Dataset/Supermarket_Data_Prediction.csv"
supermarket_data_predict <- read_csv(file_path)
View(supermarket_data_predict)
supermarket_data_predict$most_pref_shop=factor(supermarket_data_predict$most_pref_shop)
str(supermarket_data_predict)
```

__2) Stratified k-fold cross validation__

```{r echo=TRUE, message=FALSE}
#stratified k-fold(5)
flds <- createFolds(factor(supermarket_data_predict$most_pref_shop), k = 5, list = FALSE, returnTrain = TRUE)
comb_factor <- tbl_df(cbind(supermarket_data_predict, flds))
train_folders <- c(1,3,4)
test_folders <- c(2,5)
#test train data separation
train <- supermarket_data_predict[comb_factor$flds %in% train_folders,]
test <- supermarket_data_predict[comb_factor$flds %in% test_folders,]
```
__3)  Accuracy calculation__
```{r echo=TRUE, message=FALSE}
#function to calculate accuracy of different models
printALL=function(model,name){
  trainPred=predict(model,newdata =train, type = "class")
  trainTable=table(train$most_pref_shop, trainPred)
  testPred=predict(model, newdata=test, type="class")
  testTable=table(test$most_pref_shop, testPred) 
  trainAcc=(trainTable[1,1]+trainTable[2,2]+trainTable[3,3])/sum(trainTable)
  testAcc=(testTable[1,1]+testTable[2,2]+testTable[3,3])/sum(testTable)
  message(name)
  print(round(cbind(trainAccuracy=trainAcc, testAccuracy=testAcc),3))
}
```
## Classification and plotting
__4) Naive Bayes __
```{r echo=TRUE, message=FALSE}
#hypertuning isn't recommended for naive bayes because the overall performance is affected by just one parameter i.e laplace
NBclassfier=naiveBayes(most_pref_shop ~., data=train,laplace=3)
print(NBclassfier)
printALL(NBclassfier,"naive bayes")

```

__5) Decision tree (rpart) __
```{r echo=TRUE, message=FALSE}
#hypertuning rpart
obj3 <- tune.rpart(most_pref_shop~., data =train, minsplit = c(5,10,15))
summary(obj3)
#decision tree classifier
modelr<-rpart(most_pref_shop ~., data=train, method="class",control=rpart.control(cp=0.001))
print(modelr$cptable)
plotcp(modelr)
print(modelr)
prp(modelr)

#the relative error is reduced but the graph is overplotted
modelr<-rpart(most_pref_shop ~., data=train, method="class",control=rpart.control(cp=0.0001))
print(modelr$cptable)
printALL(modelr,"decision trees")
```
__Conclusion:__
By considering the features distance_avg,products_purchased_avg,unique_products_purchased_avg,product_price_Avg, amount_purchased_Avg, most preferred shop is calculated and the above mentioned classification algorithms are applied.
This will further help in predicting which shop the customer would choose